# Linear models {#linear-models}

```{r include=FALSE}
library(broom)
```

Linear regression is a powerful technique for finding a line that approximates a set of data. For the approximation to be a good one, the linear model must be appropriate for the data, which can sometimes be determined by reasoning about the processes that generate the data, and is sometimes justified based on statistical properties of the data. We will use linear models as a tool without elaboration of the methods or theoretical background; you should learn about those in a different statistics course.

We will explore how to create a linear model, which can include a lot more than straight lines, and then discuss how to add those models to a visualization.

## Making linear models

We will always make linear models with variables from a data frame. Designate one variable the response variable, which we will attempt to predict using one or more other variables, called predictors. You are not restricted to variables in your data frame; you can transform the variables first, for example by squaring, taking logarithms, or applying some other function. Additionally, you can use categorical (or factor) variables as predictors and are not restricted to only quantitative variables. Be aware that the more variables or transformations you add to your list of predictors, the more likely they will be correlated and your model will be very hard to interpret. These issues are discussed in statistics courses on regression.

Once your data frame is created, write the linear model as a "formula" object, meaning as an equation but with a `~` instead of an `=` to indicate that you are modelling the left hand side and allowing for a specific model for the mismatch between predictors and response.

We have seen that the price of a diamond increases a bit faster than linearly as the mass of the diamond increases, we will try both a linear model and a quadratic model for data in the `diamonds` dataframe.

```{r}
linear_model1 <- lm(price ~ carat, data = diamonds)
summary(linear_model1)
quadratic_model1 <- lm(price ~ carat + I(carat^2), 
                       data = diamonds %>% mutate(carat_sq = carat^2))
summary(quadratic_model1)
linear_model2 <- lm(price ~ carat + clarity + color, 
                    data = diamonds %>% mutate(clarity = factor(clarity, ordered=FALSE), 
                                               color = factor(color, ordered=FALSE)))
summary(linear_model2)
```

```{r}
library(equatiomatic)
```

You can write the formulas for the regression lines using the `equatiomatic` package. (To get the equations formatted properly, you need to add `results='asis'` to the `{r}` line in your R markdown document. The equations display correctly in the knitted document, but are show as LaTeX code in the Rstudio preview.) At present this does not work correctly for functions with mathematical transformations like `log`, `exp`, `poly(x, 2)`, so I'll only show the linear models here.

```{r results = "asis"}
extract_eq(linear_model1)
extract_eq(linear_model2, wrap=TRUE)
```

and you can fill in the results showing the numeric coefficients in the equations:

```{r results="asis"}
extract_eq(linear_model1, use_coefs=TRUE, fix_signs=TRUE)
```

```{r results="asis"}
extract_eq(linear_model2, use_coefs=TRUE, fix_signs=TRUE, wrap=TRUE)
```

These results are the jumping off point for a lot more exploration.


## Adding linear models to visualizations

In a visualization, what we really want to do is to add a line to a graph. There are easy ways to do this with ggplot that don't require you to make a separate model; instead we will just add a "smooth" geometry (`geom_smooth`) to a ggplot.

We start with a straight line.

```{r}
diamonds %>% filter(cut == "Ideal", color == "G") %>%
  ggplot(aes(x=carat, y = price)) + 
  geom_point() +
  geom_smooth(method = "lm")
```

Now try a quadratic.

```{r}
diamonds %>% filter(cut == "Ideal", color == "G") %>%
  ggplot(aes(x=carat, y = price)) + 
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ poly(x,2))
```

That's better, but it's still not great. In particular the model overshoots the observations for large diamonds. We can continue by adding a cubic term, but this game gets risky for statistical reasons you will learn in other courses. In this next example I'll switch to "robust regression" from the `MASS` package. Robust regression is designed to be less influenced by outliers, but this dataset does not demonstrate the effect that clearly because the outliers are not very pronounced. (It might be interesting to try this method on Anscobe's quartet.)

```{r}
diamonds %>% filter(cut == "Ideal", color == "G") %>%
  ggplot(aes(x=carat, y = price)) + 
  geom_point() +
  geom_smooth(method = MASS::rlm, formula = y ~ poly(x,3))
```

Robust regression is a big improvement for group 3, but has little effect on any of the other problems. (Try `model = y ~ poly(x,2)` to fix panel 2. There is no fix for the problem in panel 4.)

```{r message=FALSE}
p1 <- anscombe %>%
 pivot_longer(everything(),
   names_to = c(".value", "set"),
   names_pattern = "(.)(.)"
 ) %>% ggplot(aes(x=x, y=y)) +
  geom_point() + 
  facet_wrap(~ set) 
# p1 + geom_smooth(method="lm")
p1 + geom_smooth(method=MASS::rlm) # , formula  = y ~ poly(x,2))
```

## Non-parametric smooths

If we just want to highlight the pattern in the data, we can use a smooth that doesn't work from a simple formula, such as `loess` or `gam`.

```{r}
diamonds %>% filter(cut == "Ideal", color == "G") %>%
  ggplot(aes(x=carat, y = price)) + 
  geom_point() +
  geom_smooth(method = "loess")
```

This smooth has an "S" shape -- price increases slowly for the smallest diamonds, then quickly, then slowly again for the largest diamonds.  We get a similar result from a `gam` smooth.

```{r}
diamonds %>% filter(cut == "Ideal", color == "G") %>%
  ggplot(aes(x=carat, y = price)) + 
  geom_point() +
  geom_smooth(method = "gam")
```

## Smoothing on facets

The `geom_smooth` is especially powerful for facetted plots. The smooth is automatically computed and plotted for each facet separately. 

```{r}
diamonds %>% 
  filter(color %in% c("D", "F", "H", "J"), 
         clarity %in% c("SI1", "VS1", "IF")) %>%
  ggplot(aes(x=carat, y=price)) + 
  geom_point(aes(color=cut)) +
  facet_grid(color ~ clarity) + 
  scale_colour_viridis_d(begin=0, end =0.8) +
  geom_smooth(color = "black", size=0.75)  # uses loess
  # geom_smooth(method = "lm", formula = y ~ poly(x,2), color = "black", size = 0.5) # alternate version
```

Notice that I've moved the `color=cut` from the ggplot function to the `geom_point` function as the aesthetic for only the points. If the colour was specified in the first ggplot function, the `geom_smooth` would "inherit" this aesthetic mapping and would make a separate smooth for each cut (5 lines per panel). There are not enough data in some panels for some cuts to do a good job, so I revised the plot to only draw one smooth per facet. You should move the `color=cut` back to the ggplot call to see how the result changes.

## Predicting quantiles

As a final example, you can also try to predict quantiles of the data. Here we make a linear model for the 0.05, 0.50 (median), and 0.95 quantiles.

```{r}
diamonds %>% filter(cut == "Ideal", color == "G") %>%
  ggplot(aes(x=carat, y = price)) + 
  geom_point() +
  geom_quantile(method = rqss, formula = y ~ x,
                lambda = 1, quantiles = c(0.05, 0.5, 0.95))
```

## Quantitative model output

In the previous lesson I showed how to obtain residuals, predicted values, and uncertainty estimates from our models as data tables. The `broom` package has functions to simplify this process. Healy also shows how to use `broom` to fit many models (such as a model for each facet) in his [Section 6.6](https://socviz.co/modeling.html#grouped-analysis-and-list-columns), but I will skip those steps.

Get the coefficients (and standard errors, t-statistic, p-value and confidence intervals) from your model using `tidy`.

```{r}
tidy(linear_model1, conf.int = TRUE)
```

The second model has the effect of each level of clarity on price (relative to the base case of I1). Here's how we can plot the regression coefficients as dot plots with uncertainties.

```{r}
tidy(linear_model2, conf.int = TRUE) %>%
  filter(str_starts(term, "clarity")) %>%
  ggplot(aes(y = term, x = estimate, xmin = conf.low, xmax = conf.high)) + 
  geom_pointrange()
```

This can also be done easily using `coefplot`:

```{r}
library(coefplot)
coefplot(linear_model2, sort = "magnitude", intercept = FALSE)
```

You can get statistics for the model using `glance`:

```{r}
glance(linear_model1)
```

The `augment` function makes it easy to plot residuals and predicted values for many models (see `?augment`) in a systematic way. We can use `augment` on the model object to get a data frame with the data used in the model plus fitted (predicted) values, residuals, and other quantities.  You can add the other data not provided in the model object by specifify `data = diamonds` in the augment function.
 
```{r}
augment(linear_model1, interval = "prediction") %>% head()
# augment(linear_model1, interval = "prediction", data = diamonds) %>% head()
```

If you generate new data (along one predictor variable and using means or selected levels of other variables), you can make and plot predictions easily.

```{r}
new_data = tibble(carat = seq(0.20, 5.01, 0.01))
augment(linear_model1, newdata = new_data, interval = "prediction") %>%
  ggplot(aes(x = carat, y = .fitted, ymin = .lower, ymax = .upper)) + 
  geom_ribbon(fill="darkblue", alpha = 0.5) + 
  geom_line(color="blue")
```

Of course, this is overly complicated for plotting a straight line, but the method can be adopted to many other models.


## Further reading

* Healy [Chapter 6 Work with models](https://socviz.co/modeling.html#modeling)
* Documentation for [equationomatic](https://github.com/datalorax/equatiomatic) including instructions for installation.
